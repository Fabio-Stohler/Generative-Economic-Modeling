{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5992bf2",
   "metadata": {},
   "source": [
    "# Brock–Mirman Surrogate Analysis\n",
    "\n",
    "End-to-end walkthrough of the Brock–Mirman surrogate pipeline.\n",
    "- Colab: set `COLAB_SETUP = True` in the next cell (or open via the Colab link in the README) to auto-clone and install the repo in your session.\n",
    "- Steps covered: imports & config, data generation/loading, preprocessing, surrogate fitting, evaluation & plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc2c9e4",
   "metadata": {
    "title": "Optional: Colab setup (run first in Colab)"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "COLAB_SETUP = False  # set True in Colab to clone & install this repo\n",
    "if COLAB_SETUP:\n",
    "    REPO_URL = \"https://github.com/Fabio-Stohler/Generative-Economic-Modeling.git\"\n",
    "    BRANCH = \"main\"\n",
    "    CLONE_DIR = Path(\"/content/generative-economic-modeling\")\n",
    "\n",
    "    if CLONE_DIR.exists():\n",
    "        os.chdir(CLONE_DIR)\n",
    "        subprocess.run([\"git\", \"pull\"], check=True)\n",
    "    else:\n",
    "        subprocess.run(\n",
    "            [\"git\", \"clone\", \"--depth\", \"1\", \"--branch\", BRANCH, REPO_URL, str(CLONE_DIR)],\n",
    "            check=True,\n",
    "        )\n",
    "        os.chdir(CLONE_DIR)\n",
    "\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-e\", \".\"], check=True)\n",
    "    BASE_DIR = CLONE_DIR\n",
    "else:\n",
    "    try:\n",
    "        BASE_DIR = Path(__file__).resolve().parent.parent\n",
    "    except NameError:\n",
    "        cwd = Path.cwd()\n",
    "        BASE_DIR = cwd.parent if cwd.name == \"examples\" else cwd\n",
    "\n",
    "# Ensure source is importable without installing the package\n",
    "sys.path.insert(0, str(BASE_DIR / \"src\"))\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"bld\" / \"data\" / \"BM\"\n",
    "FIG_DIR = BASE_DIR / \"bld\" / \"figures\" / \"BM\"\n",
    "NN_DIR = BASE_DIR / \"bld\" / \"models\" / \"BM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cc6786",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Third-party dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "# Local imports\n",
    "from gem import plots\n",
    "from gem.data import (\n",
    "    ar1_lognormal_draws,\n",
    "    calculate_EEE_BM,\n",
    "    calculate_EEE_BM_Ana,\n",
    "    construct_single_xy,\n",
    "    simulate_and_save,\n",
    "    simulate_BM,\n",
    "    standardized_moments,\n",
    "    tensor_to_dataframe,\n",
    "    load_dataset,\n",
    ")\n",
    "from gem.surrogates import Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2df9c23",
   "metadata": {
    "title": "Check if CUDA is available"
   },
   "outputs": [],
   "source": [
    "Force_CPU = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if Force_CPU:\n",
    "    device = \"cpu\"\n",
    "    print(\"Forcing CPU usage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15dbc9b",
   "metadata": {},
   "source": [
    "## Run-time switches\n",
    "- `retrain`: train surrogates (True) or load cached ones (False).\n",
    "- `rerun`: regenerate simulation datasets (True) or load cached pickles (False).\n",
    "- `truncate_length`: cap dataset length for faster experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bdcbf",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Constants and lists"
   },
   "outputs": [],
   "source": [
    "retrain = False\n",
    "rerun = False\n",
    "\n",
    "# Truncation length for dataset\n",
    "truncate_length = 10000  # Set to None to use the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425b22b",
   "metadata": {},
   "source": [
    "## Reproducibility and plotting defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0dde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for torch and numpy\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda176e",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Matplotlib settings"
   },
   "outputs": [],
   "source": [
    "# Increase the font size\n",
    "plt.rcParams.update({\"font.size\": 13})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b755d9",
   "metadata": {},
   "source": [
    "## Data generation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cb3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "# Settings for the experiment in general\n",
    "experiment_settings = {\n",
    "    \"draws\": 1,\n",
    "    \"steps\": 10001,\n",
    "    \"burn\": 0,\n",
    "    \"labnorm\": True,\n",
    "    \"normalize_input\": True,\n",
    "    \"scale_output\": False,\n",
    "    \"model_save_path\": str(DATA_DIR) + \"/\",\n",
    "    \"fig_save_path\": str(FIG_DIR) + \"/\",\n",
    "    \"nn_save_path\": str(NN_DIR) + \"/\",\n",
    "}\n",
    "\n",
    "# Specify the parameters, ranges and distributions for the experiment\n",
    "# Parameters\n",
    "par = {\n",
    "    \"alpha\": 0.33,\n",
    "    \"beta\": 0.96,\n",
    "    \"omega\": 1.0,\n",
    "    \"gamma\": 5.0,\n",
    "    \"rho_a\": 0.9,\n",
    "    \"rho_z\": 0.9,\n",
    "    \"rho_mu\": 0.0,\n",
    "    \"sigma_a\": 0.05,\n",
    "    \"sigma_z\": 0.05,\n",
    "    \"sigma_mu\": 0.05,\n",
    "    \"Abar\": 1.0,\n",
    "    \"Zbar\": 1.0,\n",
    "    \"mubar\": 1.0,\n",
    "    \"tau_L\": 0.0,\n",
    "    \"tau_K\": 0.0,\n",
    "    \"tau_C\": 0.0,\n",
    "    \"kss\": 1.0,\n",
    "    \"tau_switch\": 0.0,\n",
    "}\n",
    "\n",
    "# distr_ranges for the simulation\n",
    "# Distributions\n",
    "distr_F = {\n",
    "    \"epsilon_a\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_z\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_mu\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"tau\": torch.distributions.Uniform(0.0, 1.0),\n",
    "}\n",
    "distr_AB = {\n",
    "    \"epsilon_a\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_z\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_mu\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"tau\": torch.distributions.Uniform(0.0, 1.0),\n",
    "}\n",
    "distr_AC = {\n",
    "    \"epsilon_a\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_z\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"epsilon_mu\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"tau\": torch.distributions.Uniform(0.0, 1.0),\n",
    "}\n",
    "distr_BC = {\n",
    "    \"epsilon_a\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"epsilon_z\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_mu\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"tau\": torch.distributions.Uniform(0.0, 1.0),\n",
    "}\n",
    "distr_A = {\n",
    "    \"epsilon_a\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_z\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"epsilon_mu\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"tau\": torch.distributions.Uniform(0.0, 1.0),\n",
    "}\n",
    "distr_B = {\n",
    "    \"epsilon_a\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"epsilon_z\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"epsilon_mu\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"tau\": torch.distributions.Uniform(0.0, 1.0),\n",
    "}\n",
    "distr_C = {\n",
    "    \"epsilon_a\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"epsilon_z\": torch.distributions.Normal(0.0, 1.0e-14),\n",
    "    \"epsilon_mu\": torch.distributions.Normal(0.0, 1.0),\n",
    "    \"tau\": torch.distributions.Uniform(0.0, 1.0),\n",
    "}\n",
    "\n",
    "distr_ranges = [distr_F, distr_AB, distr_AC, distr_BC, distr_A, distr_B, distr_C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa5867",
   "metadata": {},
   "source": [
    "## Data generation (simulate or load cached pickles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a6d7f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# simulate the models\n",
    "if rerun:\n",
    "    simulate_and_save(\n",
    "        par,\n",
    "        {},\n",
    "        distr_ranges,\n",
    "        experiment_settings[\"model_save_path\"],\n",
    "        [\"F\", \"AB\", \"AC\", \"BC\", \"A\", \"B\", \"C\"],\n",
    "        draws=experiment_settings[\"draws\"],\n",
    "        steps=experiment_settings[\"steps\"],\n",
    "        burn=experiment_settings[\"burn\"],\n",
    "        labnorm=experiment_settings[\"labnorm\"],\n",
    "    )\n",
    "\n",
    "# load the data into a tensor\n",
    "data_all = load_dataset(experiment_settings[\"model_save_path\"] + \"F.pkl\")\n",
    "data_AB = load_dataset(experiment_settings[\"model_save_path\"] + \"AB.pkl\")\n",
    "data_AC = load_dataset(experiment_settings[\"model_save_path\"] + \"AC.pkl\")\n",
    "data_BC = load_dataset(experiment_settings[\"model_save_path\"] + \"BC.pkl\")\n",
    "data_A = load_dataset(experiment_settings[\"model_save_path\"] + \"A.pkl\")\n",
    "data_B = load_dataset(experiment_settings[\"model_save_path\"] + \"B.pkl\")\n",
    "data_C = load_dataset(experiment_settings[\"model_save_path\"] + \"C.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36769126",
   "metadata": {},
   "source": [
    "## Preprocess datasets (reshape and merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a9501d",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Join the first and second dimension for \"x\" and \"y\" in each dataset"
   },
   "outputs": [],
   "source": [
    "for data in [data_all, data_AB, data_AC, data_BC, data_A, data_B, data_C]:\n",
    "    for key in [\"x\", \"y\"]:\n",
    "        if key in data and data[key] is not None and data[key].ndim >= 2:\n",
    "            shape = data[key].shape\n",
    "            data[key] = data[key].reshape(-1, *shape[2:]) if data[key].ndim > 2 else data[key].reshape(-1, shape[-1])\n",
    "\n",
    "\n",
    "# extract the first four variables of x, and the last two variables of y, and drop the last row\n",
    "data_all_df = pd.concat(\n",
    "    [\n",
    "        tensor_to_dataframe(data_all[\"x\"][:, :4], [\"K\", \"A\", \"Z\", \"mu\"]),\n",
    "        tensor_to_dataframe(data_all[\"y\"][:, -2:], [\"C\", \"L\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ").iloc[:-1]\n",
    "data_AB_df = pd.concat(\n",
    "    [\n",
    "        tensor_to_dataframe(data_AB[\"x\"][:, :4], [\"K\", \"A\", \"Z\", \"mu\"]),\n",
    "        tensor_to_dataframe(data_AB[\"y\"][:, -2:], [\"C\", \"L\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ").iloc[:-1]\n",
    "data_AC_df = pd.concat(\n",
    "    [\n",
    "        tensor_to_dataframe(data_AC[\"x\"][:, :4], [\"K\", \"A\", \"Z\", \"mu\"]),\n",
    "        tensor_to_dataframe(data_AC[\"y\"][:, -2:], [\"C\", \"L\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ").iloc[:-1]\n",
    "data_BC_df = pd.concat(\n",
    "    [\n",
    "        tensor_to_dataframe(data_BC[\"x\"][:, :4], [\"K\", \"A\", \"Z\", \"mu\"]),\n",
    "        tensor_to_dataframe(data_BC[\"y\"][:, -2:], [\"C\", \"L\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ").iloc[:-1]\n",
    "data_C_df = pd.concat(\n",
    "    [\n",
    "        tensor_to_dataframe(data_C[\"x\"][:, :4], [\"K\", \"A\", \"Z\", \"mu\"]),\n",
    "        tensor_to_dataframe(data_C[\"y\"][:, -2:], [\"C\", \"L\"]),\n",
    "    ],\n",
    "    axis=1,\n",
    ").iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dc0bf3",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Construct datasets for all models"
   },
   "outputs": [],
   "source": [
    "partial_datasets = {}\n",
    "print(\"Constructind dataset from all data\")\n",
    "partial_datasets[\"all\"] = construct_single_xy(data_all_df, active_eps=[\"ϵ_a\", \"ϵ_z\", \"ϵ_mu\"], extract_eps=True)\n",
    "\n",
    "print(\"Constructing dataset from tfp_zeta data\")\n",
    "partial_datasets[\"AB\"] = construct_single_xy(data_AB_df, active_eps=[\"ϵ_a\", \"ϵ_z\"], extract_eps=True)\n",
    "\n",
    "print(\"Constructing dataset from tfp_delta data\")\n",
    "partial_datasets[\"AC\"] = construct_single_xy(data_AC_df, active_eps=[\"ϵ_a\", \"ϵ_mu\"], extract_eps=True)\n",
    "\n",
    "print(\"Constructing dataset from zeta_delta data\")\n",
    "partial_datasets[\"BC\"] = construct_single_xy(data_BC_df, active_eps=[\"ϵ_z\", \"ϵ_mu\"], extract_eps=True)\n",
    "\n",
    "print(\"Constructing dataset from delta data\")\n",
    "partial_datasets[\"C\"] = construct_single_xy(data_C_df, active_eps=[\"ϵ_mu\"], extract_eps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e715205",
   "metadata": {
    "title": "Combine the datasets"
   },
   "outputs": [],
   "source": [
    "combinations = {\n",
    "    \"F\": [\"all\"],\n",
    "    \"ABC\": [\"AB\", \"AC\", \"BC\"],\n",
    "    \"C\": [\"C\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156987b4",
   "metadata": {
    "title": "Combine, shuffle, and truncate the datasets"
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for key, combination in combinations.items():\n",
    "    temp_x_list = []\n",
    "    temp_y_list = []\n",
    "\n",
    "    for name in combination:\n",
    "        # Shuffle each dataset individually\n",
    "        indices = torch.randperm(partial_datasets[name][\"x\"].shape[0])\n",
    "        shuffled_x = partial_datasets[name][\"x\"][indices]\n",
    "        shuffled_y = partial_datasets[name][\"y\"][indices]\n",
    "\n",
    "        # Truncate each dataset proportionally if truncation is enabled\n",
    "        if truncate_length is not None:\n",
    "            proportion = truncate_length // len(combination)\n",
    "            shuffled_x = shuffled_x[:proportion]\n",
    "            shuffled_y = shuffled_y[:proportion]\n",
    "\n",
    "        temp_x_list.append(shuffled_x)\n",
    "        temp_y_list.append(shuffled_y)\n",
    "\n",
    "    # Concatenate the shuffled and truncated datasets\n",
    "    temp_x = torch.cat(temp_x_list, dim=0)\n",
    "    temp_y = torch.cat(temp_y_list, dim=0)\n",
    "\n",
    "    # Perform a final shuffle to mix the datasets\n",
    "    indices = torch.randperm(temp_x.shape[0])\n",
    "    temp_x = temp_x[indices]\n",
    "    temp_y = temp_y[indices]\n",
    "\n",
    "    datasets[key] = {\"x\": temp_x, \"y\": temp_y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425765d",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Specify the neural network settings"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Settings for the neural network\n",
    "nn_settings = {\n",
    "    \"hidden\": 128,\n",
    "    \"layers\": 5,\n",
    "    \"normalize_input\": True,\n",
    "    \"scale_output\": True,\n",
    "}\n",
    "# Settings for the training of the neural network\n",
    "training_settings = {\n",
    "    \"F\": {\n",
    "        \"batch\": 100,\n",
    "        \"epochs\": 1000,\n",
    "        \"lr\": 1e-3,\n",
    "        \"eta_min\": 1e-10,\n",
    "        \"validation_share\": 0.8,\n",
    "        \"print_after\": 100,\n",
    "    },\n",
    "    \"ABC\": {\n",
    "        \"batch\": 100,\n",
    "        \"epochs\": 1000,\n",
    "        \"lr\": 1e-3,\n",
    "        \"eta_min\": 1e-10,\n",
    "        \"validation_share\": 0.8,\n",
    "        \"print_after\": 100,\n",
    "    },\n",
    "    \"C\": {\n",
    "        \"batch\": 100,\n",
    "        \"epochs\": 1000,\n",
    "        \"lr\": 1e-3,\n",
    "        \"eta_min\": 1e-10,\n",
    "        \"validation_share\": 0.8,\n",
    "        \"print_after\": 100,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33e82c",
   "metadata": {},
   "source": [
    "## Train or load surrogates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed066dda",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Training or loading the surrogate models"
   },
   "outputs": [],
   "source": [
    "models_path = str(NN_DIR) + \"/\"\n",
    "Path(models_path).mkdir(parents=True, exist_ok=True)\n",
    "surrogates = {}\n",
    "\n",
    "# Depending on the specified option, either retrain the models or load the results\n",
    "if retrain:\n",
    "    # Train the surrogate models\n",
    "    for key, data in datasets.items():\n",
    "        surrogate = Surrogate(data=data)\n",
    "        surrogate.make_network(**nn_settings)\n",
    "        surrogate.train(**training_settings[key], device=device, shuffle_data=True)\n",
    "        surrogate.save(f\"{models_path}surrogate_{key}.pkl\")\n",
    "        surrogates[key] = surrogate\n",
    "else:\n",
    "    # Load the surrogate models\n",
    "    for key in combinations.keys():\n",
    "        surrogate = Surrogate()\n",
    "        surrogate.load_attributes(f\"{models_path}surrogate_{key}.pkl\")\n",
    "        surrogates[key] = surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b0f2b",
   "metadata": {
    "title": "Move the surrogate models to CPU for plotting"
   },
   "outputs": [],
   "source": [
    "for key, surrogate in surrogates.items():\n",
    "    surrogate.network.to(\"cpu\")\n",
    "    surrogate.data_validation[\"x\"] = surrogate.data_validation[\"x\"].to(\"cpu\")\n",
    "    surrogate.data_validation[\"y\"] = surrogate.data_validation[\"y\"].to(\"cpu\")\n",
    "    surrogate.data_train[\"x\"] = surrogate.data_train[\"x\"].to(\"cpu\")\n",
    "    surrogate.data_train[\"y\"] = surrogate.data_train[\"y\"].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927e4e7e",
   "metadata": {},
   "source": [
    "## Evaluation & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be09a0b1",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "Figures"
   },
   "outputs": [],
   "source": [
    "figures_path = str(FIG_DIR) + \"/\"\n",
    "figure_suffix = \"\"\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51391738",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "surrogate_labels = {\n",
    "    \"F\": \"Full\",\n",
    "    \"ABC\": \"Glued\",\n",
    "    \"C\": \"Mu only\",\n",
    "}\n",
    "smoothing_window = 10\n",
    "\n",
    "# plot the training and the validation loss for all but the true surrogate\n",
    "for key, surrogate in [(k, v) for k, v in surrogates.items() if k != \"True\"]:\n",
    "    loss_dict = surrogate.loss_dict\n",
    "    save_name = f\"loss_curves_{key}{figure_suffix}\"\n",
    "    fig, ax = plots.plot_training_validation_loss(\n",
    "        loss_dict,\n",
    "        label=surrogate_labels[key],\n",
    "        ylim=min(np.minimum(loss_dict[\"training_loss\"], loss_dict[\"validation_loss\"])),\n",
    "        smoothing_window=smoothing_window,\n",
    "        selected_keys=[\n",
    "            \"training_loss\",\n",
    "            \"validation_loss\",\n",
    "        ],\n",
    "        save_path=figures_path,\n",
    "        save_name=save_name,\n",
    "        locators=False,\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with three surrogates\n",
    "plots.plot_error_histogram_three(\n",
    "    surrogate1=surrogates[\"C\"],\n",
    "    surrogate2=surrogates[\"F\"],\n",
    "    surrogate3=surrogates[\"ABC\"],\n",
    "    x_validation=surrogates[\"F\"].data_validation[\"x\"],\n",
    "    y_validation=surrogates[\"F\"].data_validation[\"y\"],\n",
    "    y_labels=[\"K\", \"C\", \"L\"],\n",
    "    plot_vars=[\"K\"],\n",
    "    save_name=\"error_histogram_three\",\n",
    "    save_path=figures_path,\n",
    "    ncol=1,\n",
    "    sub_figsize=(6, 4),\n",
    "    x_range=(-0.1, 0.1),\n",
    "    maxpercent=0.5,\n",
    "    percentsteps=0.1,\n",
    "    Latex_label=[\n",
    "        r\"Histogram of the Approximation Error of $K_t$\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b50d57",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "example with the other two variables"
   },
   "outputs": [],
   "source": [
    "plots.plot_error_histogram_three(\n",
    "    surrogate1=surrogates[\"C\"],\n",
    "    surrogate2=surrogates[\"F\"],\n",
    "    surrogate3=surrogates[\"ABC\"],\n",
    "    x_validation=surrogates[\"F\"].data_validation[\"x\"],\n",
    "    y_validation=surrogates[\"F\"].data_validation[\"y\"],\n",
    "    y_labels=[\"K\", \"C\", \"L\"],\n",
    "    plot_vars=[\"C\", \"L\"],\n",
    "    save_name=\"error_histogram_three_other_variables\",\n",
    "    save_path=figures_path,\n",
    "    ncol=2,\n",
    "    sub_figsize=(6, 4),\n",
    "    x_range=(-0.75, 0.75),\n",
    "    maxpercent=0.5,\n",
    "    percentsteps=0.1,\n",
    "    Latex_label=[r\"$C_t$\", r\"$L_t$\"],\n",
    ")\n",
    "\n",
    "plots.plot_error_histogram_three(\n",
    "    surrogate1=surrogates[\"C\"],\n",
    "    surrogate2=surrogates[\"F\"],\n",
    "    surrogate3=surrogates[\"ABC\"],\n",
    "    x_validation=surrogates[\"F\"].data_validation[\"x\"],\n",
    "    y_validation=surrogates[\"F\"].data_validation[\"y\"],\n",
    "    y_labels=[\"K\", \"C\", \"L\"],\n",
    "    plot_vars=[\"C\", \"L\"],\n",
    "    save_name=\"error_histogram_three_other_variables_zoomed\",\n",
    "    save_path=figures_path,\n",
    "    ncol=2,\n",
    "    sub_figsize=(6, 4),\n",
    "    x_range=(-0.003, 0.003),\n",
    "    maxpercent=0.5,\n",
    "    percentsteps=0.1,\n",
    "    Latex_label=[r\"$C_t$\", r\"$L_t$\"],\n",
    ")\n",
    "\n",
    "\n",
    "# extracting the shocks from the full dataset\n",
    "exo_shocks_BM = data_all[\"x\"][:, 1:4]\n",
    "\n",
    "# extract the shocks into a dataframe\n",
    "shocks_df = pd.DataFrame(exo_shocks_BM.cpu().numpy(), columns=[\"A\", \"Z\", \"mu\"])\n",
    "rhos = torch.tensor([0.9, 0.9, 0.0], device=device, dtype=torch.float32)\n",
    "log_means = torch.log(torch.tensor([par[\"Abar\"], par[\"Zbar\"], par[\"mubar\"]], device=device, dtype=torch.float32))\n",
    "stds = torch.tensor([0.05, 0.05, 0.05], device=device)\n",
    "log_means[2] = stds[2] ** 2  # set mean of mu to 1\n",
    "\n",
    "# fix the seed for reproducibility\n",
    "torch.manual_seed(420)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(420)\n",
    "\n",
    "# number of Monte Carlo simulations\n",
    "N_monte_carlo = 5000\n",
    "\n",
    "# simulate the shocks\n",
    "eps = torch.randn(N_monte_carlo // 2, 3, device=device)\n",
    "\n",
    "# mirror the same shocks again (antithetic)\n",
    "eps = torch.cat((eps, -eps), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b7f49",
   "metadata": {},
   "source": [
    "## Set up shocks and states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b31150",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "predict = [\"Kp\", \"C\", \"L\"]\n",
    "states = [\"K\", \"A\", \"Z\", \"mu\"]\n",
    "\n",
    "# transform the parameters to tensors\n",
    "device = eps.device\n",
    "dtype = eps.dtype\n",
    "means = torch.as_tensor(data_all[\"x\"][0, 1:4], device=device, dtype=dtype)\n",
    "rhos_t = torch.as_tensor(rhos, device=device, dtype=dtype)\n",
    "stds_t = torch.as_tensor(stds, device=device, dtype=dtype)\n",
    "\n",
    "# static evaluation of the Euler Equation Error at the mean state\n",
    "EEE_static = {}\n",
    "EEE_analytical = {}\n",
    "print(\"Static Euler Equation Error at the mean state:\")\n",
    "for name in [\"F\", \"ABC\"]:\n",
    "    todays_state = data_all[\"x\"][6, :4].to(device)\n",
    "    future_exo_states = ar1_lognormal_draws(states=todays_state[1:], rhos=rhos, stds=stds, shocks=eps, log_means=log_means)  # A, Z, mu today\n",
    "    EEE_analytical[name] = calculate_EEE_BM_Ana(todays_state, future_exo_states, par)\n",
    "    EEE_static[name] = calculate_EEE_BM(todays_state, surrogates[name], future_exo_states, par)\n",
    "    print(f\"BM EEE of {name} at mean state: {EEE_static[name]:.5f}\")\n",
    "    print(f\"Analytical EEE of {name} at mean state: {EEE_analytical[name]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c1c18",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "simulate the economy forward for N_sim periods and calculate the EEE for each period"
   },
   "outputs": [],
   "source": [
    "N_sim = 1000  # lower number than in the paper, but for faster runtime\n",
    "\n",
    "# simulating forward\n",
    "print(f\"Dynamic Euler Equation Error for {N_sim} periods:\")\n",
    "EEE = {}\n",
    "\n",
    "# simulate the economy forward\n",
    "for name in [\"F\", \"ABC\"]:\n",
    "    # initialize the state with the mean state from the full dataset\n",
    "    todays_state = data_all[\"x\"][0, :4].to(device)\n",
    "\n",
    "    # initialize lists to store the results\n",
    "    EEE[name] = []\n",
    "\n",
    "    # get the surrogate model\n",
    "    surrogate = surrogates[name]\n",
    "\n",
    "    # simulating the economy forward for N_sim periods\n",
    "    for t in trange(N_sim - 1):\n",
    "        # simulate the shocks\n",
    "        shocks = torch.randn(N_monte_carlo // 2, 3, device=device)\n",
    "\n",
    "        # mirror the same shocks again (antithetic)\n",
    "        shocks = torch.cat((shocks, -shocks), dim=0)\n",
    "\n",
    "        # draw the shocks for the next period to calculate the EEE\n",
    "        exo_shocks = ar1_lognormal_draws(\n",
    "            states=todays_state[1:],\n",
    "            rhos=rhos,\n",
    "            stds=stds,\n",
    "            shocks=eps,\n",
    "            log_means=log_means,\n",
    "        )\n",
    "\n",
    "        # calculate the EEE for the current period\n",
    "        EEE_t = calculate_EEE_BM(todays_state, surrogates[name], exo_shocks, par)\n",
    "\n",
    "        # store the EEE in a list\n",
    "        EEE[name].append(EEE_t)\n",
    "\n",
    "        # simulate the economy one period ahead\n",
    "        next_state = simulate_BM(todays_state, surrogates[name], exo_shocks_BM[t + 1, :], device=device).squeeze()\n",
    "\n",
    "        # update the todays_state variable\n",
    "        todays_state = next_state.detach().squeeze().clone()\n",
    "\n",
    "    # print the average EEE over the simulated periods\n",
    "    print(f\"Average BM EEE of {name} over {N_sim} periods: {np.mean(EEE[name]):.5f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6cb7f",
   "metadata": {
    "lines_to_next_cell": 2,
    "title": "plot the EEE over time"
   },
   "outputs": [],
   "source": [
    "importlib.reload(plots)\n",
    "plots.plot_euler_error_histogram(\n",
    "    EEE[\"F\"],\n",
    "    EEE[\"ABC\"],\n",
    "    save_path=figures_path,\n",
    "    save_name=\"dynamic_euler_error_histogram\",\n",
    "    number_bins=50,\n",
    "    num_locators_x=5,\n",
    "    num_locators_y=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093884ab",
   "metadata": {
    "title": "simulate the model forward using the exogenous processes of the simulations"
   },
   "outputs": [],
   "source": [
    "Capital = {}\n",
    "# simulate the economy forward\n",
    "for name, data in [\n",
    "    (\"F\", data_all[\"x\"][:, :4].to(device)),\n",
    "    (\"ABC\", data_all[\"x\"][:, :4].to(device)),\n",
    "    (\"C\", data_C[\"x\"][:, :4].to(device)),\n",
    "]:\n",
    "    # initialize the state with the mean state from the full dataset\n",
    "    todays_state = data[0, :4].to(device)\n",
    "\n",
    "    # load the surrogate model\n",
    "    surrogate = surrogates[name]\n",
    "\n",
    "    # initialize lists to store the results\n",
    "    Capital[name] = []\n",
    "\n",
    "    # simulating the economy forward for N_sim periods\n",
    "    for t in trange(N_sim - 1):\n",
    "        # store the capital in a list\n",
    "        Capital[name].append(todays_state[0].item())\n",
    "\n",
    "        # simulate the economy one period ahead\n",
    "        next_state = simulate_BM(todays_state, surrogates[name], data[t + 1, 1:4], device=device).squeeze()\n",
    "\n",
    "        # update the todays_state variable\n",
    "        todays_state = next_state.detach().squeeze().clone()\n",
    "\n",
    "\n",
    "# Print standardized moments as a formatted table\n",
    "moment_names = [\"Mean\", \"Std\", \"Skewness\", \"Kurtosis\"]\n",
    "results = []\n",
    "for name in [\"F\", \"ABC\", \"C\"]:\n",
    "    moments = standardized_moments(Capital[name])\n",
    "    results.append([name] + [f\"{m:.4f}\" for m in moments])\n",
    "\n",
    "df_moments = pd.DataFrame(results, columns=[\"Model\"] + moment_names)\n",
    "print(df_moments.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202783b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
